# DQN(Deep Q-Network)이란?

dqn은 기존의 Q-Learning 알고리즘을 신경망으로 변경한 것.(Q-table -> 신경망)

행동의 개수가 한정적이고 적은 개수라면 Q-table을 만들어가면 되지만, 무한하다고 할 만큼 수많은 행동이 존재하는 환경에서는 모든 경우에 다 맞춰 Q-table을 최적화하는 것은 불가능에 가까움.

dqn이 나오기 전에 신경망을 강화학습에 적용하면서 발생한 몇 가지 문제를 개선한 방법임.

    문제 1. 강화학습은 연속된 state에 따라 의존적이라 state간의 상관관계가 큼. 
            연속된 경험을 학습할 때 초반의 몇 가지 경험 패턴에만 치중되어 최적의 행동 패턴을 찾기 어려움.

    문제 2. 새로운 경혐을 이전 경험에 덮어쓰게 되면서 쉽게 잊어버림.

→ menorize 기능 등장.

memory에 경험을 충분히 쌓고 그 이전 경험들을 학습하는 과정에서 무작위로 뽑아 경험 간의 상관관계를 줄임.

